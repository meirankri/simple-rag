{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "# # Create embeddingsclear\n",
    "\n",
    "\n",
    "# # Create Ollama language model - Gemma 2\n",
    "\n",
    "\n",
    "# Create the RAG chain using LCEL with prompt printing and streaming output\n",
    "\n",
    "# Function to ask questions\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables using os.getenv\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meir/Documents/dev/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = Chroma(persist_directory=\"./genese\",\n",
    "            embedding_function=embeddings)\n",
    "\n",
    "# # Create retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs= {\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_llm = 'gemma2'\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "# llm = ChatOllama(model=local_llm,\n",
    "#                  keep_alive=\"3h\", \n",
    "#                  max_tokens=512,  \n",
    "#                  temperature=0)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"<bos><start_of_turn>user\\nAnswer the question based only on the following context and extract out a meaningful answer. \\\n",
    "Please write in full sentences with correct spelling and punctuation. if it makes sense use lists. \\\n",
    "If the context doen't contain the answer, just respond that you are unable to find an answer. \\\n",
    "Use the question language to respond. \\\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\\n\n",
    "ANSWER:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it is clear that Sichem, the son of Hamor, had an illicit relationship with Dinah, the daughter of Jacob.\n",
      "\n",
      "The relevant passages that support this answer are:\n",
      "\n",
      "* \"34.2 Son coeur s'attacha à Dina, fille de Jacob; il aima la jeune fille, et sut parler à son coeur.\" (This passage indicates that Sichem had fallen in love with Dinah and was thinking about her.)\n",
      "* \"34.4 Et Sichem dit à Hamor, son père: Donne-moi cette jeune fille pour femme.\" (This passage shows that Sichem wanted to marry Dinah.)\n",
      "* \"34.7 Et les fils de Jacob revenaient des champs, lorsqu'ils apprirent la chose; ces hommes furent irrités et se mirent dans une grande colère, parce que Sichem avait commis une infamie en Israël, en couchant avec la fille de Jacob, ce qui n'aurait pas dû se faire.\" (This passage reveals that Sichem had committed an egregious sin by having an illicit relationship with Dinah.)\n",
      "\n",
      "Therefore, the answer to the question is that Sichem had an illicit relationship with Dinah.\n",
      "\n",
      "Answer:\n",
      "\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 17.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I found the following information relevant to the question:\n",
      "\n",
      "The context consists of biblical passages from the book of Genesis, specifically chapters 25, 9, 10, 28, 46, and 10.19.\n",
      "\n",
      "Unfortunately, I was unable to find a direct answer to the question in the provided context. The context primarily discusses the genealogy of various biblical figures, including Ismaël, Isaac, Noé, and Jacob, as well as their descendants. It does not contain information that would directly answer the question.\n",
      "\n",
      "If you could provide more context or clarify the question, I would be happy to try and assist you further."
     ]
    }
   ],
   "source": [
    "def ask_question(question):\n",
    "    print(\"Answer:\\n\\n\", end=\" \", flush=True)\n",
    "    for chunk in rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n",
    "        if user_question.lower() == 'quit':\n",
    "            break\n",
    "        answer = ask_question(user_question)\n",
    "        # print(\"\\nFull answer received.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
