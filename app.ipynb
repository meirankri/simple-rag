{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "# Function to ask questions\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables using os.getenv\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db = Chroma(persist_directory=\"./db-garanties\",\n",
    "            embedding_function=embeddings)\n",
    "\n",
    "# # Create retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs= {\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_llm = 'gemma2'\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "# llm = ChatOllama(model=local_llm,\n",
    "#                  keep_alive=\"3h\", \n",
    "#                  max_tokens=512,  \n",
    "#                  temperature=0)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"<bos><start_of_turn>user\\nAnswer the question based only on the following context and extract out a meaningful answer. \\\n",
    "Please write in full sentences with correct spelling and punctuation. if it makes sense use lists. \\\n",
    "If the context doen't contain the answer, just respond that you are unable to find an answer. \\\n",
    "Use the question language to respond. \\\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\\n\n",
    "ANSWER:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    print(\"Answer:\\n\\n\", end=\" \", flush=True)\n",
    "    for chunk in rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n",
    "        if user_question.lower() == 'quit':\n",
    "            break\n",
    "        answer = ask_question(user_question)\n",
    "        # print(\"\\nFull answer received.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
